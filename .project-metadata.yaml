name: LLAMA-2-7B Standalone Model Deployment
description: >-
This AMP deploys Llama-2-7b model as a CML model endpoint, callable via an API. Model is hosted within CML and requires GPU node with 16GB memory and 4 cores minimum.
author: Cloudera
date: "2023-11-15"
specification_version: 1.0
prototype_version: 1.0

environment_variables:
  MODEL_NAME:
    default: "LLAMA-2-7B CML Deployment"

runtimes: 
  - editor: PBJ Workbench
    kernel: Python 3.9
    edition: Nvidia GPU
    version: 2023.08.2-b8
  
tasks:
  - type: create_model
    name: Llama-2-7b
    entity_label: Llama-2-7b
    description: LLAMA-2-7B model hosted in CML. 
    short_summary: LLAMA-2-7B
    default_resources:
    cpu: 4
    memory: 16
    gpu: 1
  
  - type: build_model
    entity_label: Llama-2-7b
    comment: First build by the AMP
    examples:
      - request:
          prompt: What is Cloudera?

  - type: deploy_model
    entity_label: Llama-2-7b